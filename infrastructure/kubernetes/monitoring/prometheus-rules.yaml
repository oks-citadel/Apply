# Prometheus Alert Rules for JobPilot AI Platform
# This file defines comprehensive monitoring alerts using the PrometheusRule CRD
# These rules are evaluated by Prometheus and trigger alerts to AlertManager
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: jobpilot-alerts
  namespace: jobpilot
  labels:
    app: prometheus
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
  # ============================================================================
  # Group 1: Service Health Alerts
  # Monitor the health and performance of all JobPilot microservices
  # ============================================================================
  - name: service-health
    interval: 30s
    rules:
    # Critical: Service is completely down
    - alert: ServiceDown
      expr: up{job=~"kubernetes-pods|kubernetes-services",namespace="jobpilot"} == 0
      for: 1m
      labels:
        severity: critical
        category: availability
        team: platform
      annotations:
        summary: "Service {{ $labels.job }} is down"
        description: "Service {{ $labels.job }} ({{ $labels.kubernetes_pod_name }}) in namespace {{ $labels.namespace }} has been down for more than 1 minute."
        runbook_url: "https://runbooks.jobpilot.com/alerts/ServiceDown"
        dashboard_url: "https://grafana.jobpilot.com/d/service-health"

    # Warning: High rate of 5xx errors
    - alert: HighErrorRate
      expr: |
        (
          sum(rate(http_requests_total{status=~"5..",namespace="jobpilot"}[5m])) by (service, namespace)
          /
          sum(rate(http_requests_total{namespace="jobpilot"}[5m])) by (service, namespace)
        ) * 100 > 5
      for: 5m
      labels:
        severity: warning
        category: reliability
        team: platform
      annotations:
        summary: "High error rate on {{ $labels.service }}"
        description: "Service {{ $labels.service }} is experiencing {{ $value | humanizePercentage }} error rate (5xx responses) over the last 5 minutes."
        runbook_url: "https://runbooks.jobpilot.com/alerts/HighErrorRate"
        dashboard_url: "https://grafana.jobpilot.com/d/service-errors"

    # Warning: High P95 latency
    - alert: HighLatency
      expr: |
        histogram_quantile(0.95,
          sum(rate(http_request_duration_seconds_bucket{namespace="jobpilot"}[5m])) by (service, le)
        ) > 2
      for: 5m
      labels:
        severity: warning
        category: performance
        team: platform
      annotations:
        summary: "High latency on {{ $labels.service }}"
        description: "Service {{ $labels.service }} has P95 latency of {{ $value | humanizeDuration }} (threshold: 2s) over the last 5 minutes."
        runbook_url: "https://runbooks.jobpilot.com/alerts/HighLatency"
        dashboard_url: "https://grafana.jobpilot.com/d/service-latency"

    # Warning: Service using high memory
    - alert: ServiceHighMemory
      expr: |
        (
          container_memory_working_set_bytes{namespace="jobpilot",container!=""}
          /
          container_spec_memory_limit_bytes{namespace="jobpilot",container!=""}
        ) * 100 > 85
      for: 5m
      labels:
        severity: warning
        category: resources
        team: platform
      annotations:
        summary: "High memory usage on {{ $labels.pod }}"
        description: "Container {{ $labels.container }} in pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of its memory limit."
        runbook_url: "https://runbooks.jobpilot.com/alerts/ServiceHighMemory"
        dashboard_url: "https://grafana.jobpilot.com/d/service-resources"

    # Warning: Service using high CPU
    - alert: ServiceHighCPU
      expr: |
        (
          sum(rate(container_cpu_usage_seconds_total{namespace="jobpilot",container!=""}[5m])) by (pod, container)
          /
          sum(container_spec_cpu_quota{namespace="jobpilot",container!=""}/container_spec_cpu_period{namespace="jobpilot",container!=""}) by (pod, container)
        ) * 100 > 85
      for: 5m
      labels:
        severity: warning
        category: resources
        team: platform
      annotations:
        summary: "High CPU usage on {{ $labels.pod }}"
        description: "Container {{ $labels.container }} in pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of its CPU limit."
        runbook_url: "https://runbooks.jobpilot.com/alerts/ServiceHighCPU"
        dashboard_url: "https://grafana.jobpilot.com/d/service-resources"

  # ============================================================================
  # Group 2: Infrastructure Alerts
  # Monitor Kubernetes infrastructure, pods, nodes, and storage
  # ============================================================================
  - name: infrastructure
    interval: 30s
    rules:
    # Warning: Pod is crash looping
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total{namespace="jobpilot"}[5m]) > 0
      for: 5m
      labels:
        severity: warning
        category: stability
        team: platform
      annotations:
        summary: "Pod {{ $labels.pod }} is crash looping"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has restarted {{ $value }} times in the last 5 minutes."
        runbook_url: "https://runbooks.jobpilot.com/alerts/PodCrashLooping"
        dashboard_url: "https://grafana.jobpilot.com/d/pod-health"

    # Warning: Pod not ready
    - alert: PodNotReady
      expr: |
        sum by (namespace, pod) (
          kube_pod_status_phase{namespace="jobpilot",phase!~"Succeeded|Running"}
        ) > 0
      for: 5m
      labels:
        severity: warning
        category: availability
        team: platform
      annotations:
        summary: "Pod {{ $labels.pod }} is not ready"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has been in a non-ready state for more than 5 minutes."
        runbook_url: "https://runbooks.jobpilot.com/alerts/PodNotReady"
        dashboard_url: "https://grafana.jobpilot.com/d/pod-health"

    # Warning: Deployment replica mismatch
    - alert: DeploymentReplicaMismatch
      expr: |
        kube_deployment_spec_replicas{namespace="jobpilot"}
        !=
        kube_deployment_status_replicas_available{namespace="jobpilot"}
      for: 10m
      labels:
        severity: warning
        category: availability
        team: platform
      annotations:
        summary: "Deployment {{ $labels.deployment }} has mismatched replicas"
        description: "Deployment {{ $labels.deployment }} in namespace {{ $labels.namespace }} has {{ $value }} mismatched replicas (desired vs available) for more than 10 minutes."
        runbook_url: "https://runbooks.jobpilot.com/alerts/DeploymentReplicaMismatch"
        dashboard_url: "https://grafana.jobpilot.com/d/deployments"

    # Warning: PersistentVolumeClaim nearly full
    - alert: PVCNearlyFull
      expr: |
        (
          kubelet_volume_stats_used_bytes{namespace="jobpilot"}
          /
          kubelet_volume_stats_capacity_bytes{namespace="jobpilot"}
        ) * 100 > 90
      for: 5m
      labels:
        severity: warning
        category: capacity
        team: platform
      annotations:
        summary: "PVC {{ $labels.persistentvolumeclaim }} is nearly full"
        description: "PersistentVolumeClaim {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is {{ $value | humanizePercentage }} full."
        runbook_url: "https://runbooks.jobpilot.com/alerts/PVCNearlyFull"
        dashboard_url: "https://grafana.jobpilot.com/d/storage"

    # Critical: Node not ready
    - alert: NodeNotReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 5m
      labels:
        severity: critical
        category: infrastructure
        team: platform
      annotations:
        summary: "Node {{ $labels.node }} is not ready"
        description: "Kubernetes node {{ $labels.node }} has been in NotReady state for more than 5 minutes."
        runbook_url: "https://runbooks.jobpilot.com/alerts/NodeNotReady"
        dashboard_url: "https://grafana.jobpilot.com/d/nodes"

    # Warning: Node high memory pressure
    - alert: NodeMemoryPressure
      expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
      for: 5m
      labels:
        severity: warning
        category: resources
        team: platform
      annotations:
        summary: "Node {{ $labels.node }} has memory pressure"
        description: "Kubernetes node {{ $labels.node }} is experiencing memory pressure for more than 5 minutes."
        runbook_url: "https://runbooks.jobpilot.com/alerts/NodeMemoryPressure"
        dashboard_url: "https://grafana.jobpilot.com/d/nodes"

    # Warning: Node high disk pressure
    - alert: NodeDiskPressure
      expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 5m
      labels:
        severity: warning
        category: resources
        team: platform
      annotations:
        summary: "Node {{ $labels.node }} has disk pressure"
        description: "Kubernetes node {{ $labels.node }} is experiencing disk pressure for more than 5 minutes."
        runbook_url: "https://runbooks.jobpilot.com/alerts/NodeDiskPressure"
        dashboard_url: "https://grafana.jobpilot.com/d/nodes"

  # ============================================================================
  # Group 3: Database Alerts
  # Monitor PostgreSQL and Redis health and performance
  # ============================================================================
  - name: database
    interval: 30s
    rules:
    # Critical: PostgreSQL is down
    - alert: PostgresDown
      expr: pg_up{namespace="jobpilot"} == 0
      for: 1m
      labels:
        severity: critical
        category: database
        team: platform
      annotations:
        summary: "PostgreSQL instance {{ $labels.instance }} is down"
        description: "PostgreSQL database {{ $labels.instance }} in namespace {{ $labels.namespace }} has been down for more than 1 minute."
        runbook_url: "https://runbooks.jobpilot.com/alerts/PostgresDown"
        dashboard_url: "https://grafana.jobpilot.com/d/postgres"

    # Warning: PostgreSQL high connection usage
    - alert: PostgresHighConnections
      expr: |
        (
          sum(pg_stat_database_numbackends{namespace="jobpilot"}) by (instance)
          /
          pg_settings_max_connections{namespace="jobpilot"}
        ) * 100 > 80
      for: 5m
      labels:
        severity: warning
        category: database
        team: platform
      annotations:
        summary: "PostgreSQL {{ $labels.instance }} has high connection usage"
        description: "PostgreSQL instance {{ $labels.instance }} is using {{ $value | humanizePercentage }} of available connections."
        runbook_url: "https://runbooks.jobpilot.com/alerts/PostgresHighConnections"
        dashboard_url: "https://grafana.jobpilot.com/d/postgres"

    # Warning: PostgreSQL replication lag
    - alert: PostgresReplicationLag
      expr: |
        (
          pg_replication_lag{namespace="jobpilot"}
        ) > 30
      for: 5m
      labels:
        severity: warning
        category: database
        team: platform
      annotations:
        summary: "PostgreSQL replication lag on {{ $labels.instance }}"
        description: "PostgreSQL instance {{ $labels.instance }} has replication lag of {{ $value | humanizeDuration }} (threshold: 30s)."
        runbook_url: "https://runbooks.jobpilot.com/alerts/PostgresReplicationLag"
        dashboard_url: "https://grafana.jobpilot.com/d/postgres-replication"

    # Warning: PostgreSQL high transaction rate
    - alert: PostgresHighTransactionRate
      expr: rate(pg_stat_database_xact_commit{namespace="jobpilot"}[5m]) > 1000
      for: 10m
      labels:
        severity: warning
        category: database
        team: platform
      annotations:
        summary: "PostgreSQL {{ $labels.instance }} has high transaction rate"
        description: "PostgreSQL instance {{ $labels.instance }} is processing {{ $value }} transactions per second."
        runbook_url: "https://runbooks.jobpilot.com/alerts/PostgresHighTransactionRate"
        dashboard_url: "https://grafana.jobpilot.com/d/postgres"

    # Critical: Redis is down
    - alert: RedisDown
      expr: redis_up{namespace="jobpilot"} == 0
      for: 1m
      labels:
        severity: critical
        category: cache
        team: platform
      annotations:
        summary: "Redis instance {{ $labels.instance }} is down"
        description: "Redis cache {{ $labels.instance }} in namespace {{ $labels.namespace }} has been down for more than 1 minute."
        runbook_url: "https://runbooks.jobpilot.com/alerts/RedisDown"
        dashboard_url: "https://grafana.jobpilot.com/d/redis"

    # Warning: Redis high memory usage
    - alert: RedisHighMemory
      expr: |
        (
          redis_memory_used_bytes{namespace="jobpilot"}
          /
          redis_memory_max_bytes{namespace="jobpilot"}
        ) * 100 > 90
      for: 5m
      labels:
        severity: warning
        category: cache
        team: platform
      annotations:
        summary: "Redis {{ $labels.instance }} has high memory usage"
        description: "Redis instance {{ $labels.instance }} is using {{ $value | humanizePercentage }} of its maximum memory."
        runbook_url: "https://runbooks.jobpilot.com/alerts/RedisHighMemory"
        dashboard_url: "https://grafana.jobpilot.com/d/redis"

    # Warning: Redis high eviction rate
    - alert: RedisHighEvictionRate
      expr: rate(redis_evicted_keys_total{namespace="jobpilot"}[5m]) > 10
      for: 5m
      labels:
        severity: warning
        category: cache
        team: platform
      annotations:
        summary: "Redis {{ $labels.instance }} has high eviction rate"
        description: "Redis instance {{ $labels.instance }} is evicting {{ $value }} keys per second due to memory pressure."
        runbook_url: "https://runbooks.jobpilot.com/alerts/RedisHighEvictionRate"
        dashboard_url: "https://grafana.jobpilot.com/d/redis"

    # Warning: Redis rejected connections
    - alert: RedisRejectedConnections
      expr: rate(redis_rejected_connections_total{namespace="jobpilot"}[5m]) > 0
      for: 5m
      labels:
        severity: warning
        category: cache
        team: platform
      annotations:
        summary: "Redis {{ $labels.instance }} is rejecting connections"
        description: "Redis instance {{ $labels.instance }} is rejecting {{ $value }} connections per second."
        runbook_url: "https://runbooks.jobpilot.com/alerts/RedisRejectedConnections"
        dashboard_url: "https://grafana.jobpilot.com/d/redis"

  # ============================================================================
  # Group 4: Message Queue Alerts
  # Monitor RabbitMQ health and queue performance
  # ============================================================================
  - name: message-queue
    interval: 30s
    rules:
    # Critical: RabbitMQ is down
    - alert: RabbitMQDown
      expr: rabbitmq_up{namespace="jobpilot"} == 0
      for: 1m
      labels:
        severity: critical
        category: messaging
        team: platform
      annotations:
        summary: "RabbitMQ instance {{ $labels.instance }} is down"
        description: "RabbitMQ message broker {{ $labels.instance }} in namespace {{ $labels.namespace }} has been down for more than 1 minute."
        runbook_url: "https://runbooks.jobpilot.com/alerts/RabbitMQDown"
        dashboard_url: "https://grafana.jobpilot.com/d/rabbitmq"

    # Warning: RabbitMQ high queue length
    - alert: RabbitMQHighQueue
      expr: rabbitmq_queue_messages{namespace="jobpilot"} > 10000
      for: 5m
      labels:
        severity: warning
        category: messaging
        team: platform
      annotations:
        summary: "RabbitMQ queue {{ $labels.queue }} has high message count"
        description: "RabbitMQ queue {{ $labels.queue }} has {{ $value }} messages pending for more than 5 minutes. This may indicate consumer issues or high load."
        runbook_url: "https://runbooks.jobpilot.com/alerts/RabbitMQHighQueue"
        dashboard_url: "https://grafana.jobpilot.com/d/rabbitmq-queues"

    # Critical: RabbitMQ consumer down
    - alert: RabbitMQConsumerDown
      expr: rabbitmq_queue_consumers{namespace="jobpilot"} == 0
      for: 5m
      labels:
        severity: critical
        category: messaging
        team: platform
      annotations:
        summary: "RabbitMQ queue {{ $labels.queue }} has no consumers"
        description: "RabbitMQ queue {{ $labels.queue }} has no active consumers for more than 5 minutes. Messages are not being processed."
        runbook_url: "https://runbooks.jobpilot.com/alerts/RabbitMQConsumerDown"
        dashboard_url: "https://grafana.jobpilot.com/d/rabbitmq-queues"

    # Warning: RabbitMQ high unacknowledged messages
    - alert: RabbitMQHighUnackedMessages
      expr: rabbitmq_queue_messages_unacked{namespace="jobpilot"} > 1000
      for: 10m
      labels:
        severity: warning
        category: messaging
        team: platform
      annotations:
        summary: "RabbitMQ queue {{ $labels.queue }} has high unacknowledged messages"
        description: "RabbitMQ queue {{ $labels.queue }} has {{ $value }} unacknowledged messages for more than 10 minutes."
        runbook_url: "https://runbooks.jobpilot.com/alerts/RabbitMQHighUnackedMessages"
        dashboard_url: "https://grafana.jobpilot.com/d/rabbitmq-queues"

    # Warning: RabbitMQ high connection count
    - alert: RabbitMQHighConnections
      expr: rabbitmq_connections{namespace="jobpilot"} > 1000
      for: 5m
      labels:
        severity: warning
        category: messaging
        team: platform
      annotations:
        summary: "RabbitMQ has high connection count"
        description: "RabbitMQ instance {{ $labels.instance }} has {{ $value }} active connections."
        runbook_url: "https://runbooks.jobpilot.com/alerts/RabbitMQHighConnections"
        dashboard_url: "https://grafana.jobpilot.com/d/rabbitmq"

    # Critical: RabbitMQ node down
    - alert: RabbitMQNodeDown
      expr: rabbitmq_running{namespace="jobpilot"} == 0
      for: 1m
      labels:
        severity: critical
        category: messaging
        team: platform
      annotations:
        summary: "RabbitMQ node {{ $labels.node }} is down"
        description: "RabbitMQ cluster node {{ $labels.node }} has been down for more than 1 minute."
        runbook_url: "https://runbooks.jobpilot.com/alerts/RabbitMQNodeDown"
        dashboard_url: "https://grafana.jobpilot.com/d/rabbitmq-cluster"

  # ============================================================================
  # Group 5: Business Metrics Alerts
  # Monitor application-specific business KPIs and user experience
  # ============================================================================
  - name: business-metrics
    interval: 1m
    rules:
    # Warning: Low application success rate
    - alert: LowApplicationSuccessRate
      expr: |
        (
          sum(rate(job_applications_total{status="success",namespace="jobpilot"}[30m]))
          /
          sum(rate(job_applications_total{namespace="jobpilot"}[30m]))
        ) * 100 < 70
      for: 30m
      labels:
        severity: warning
        category: business
        team: product
      annotations:
        summary: "Low job application success rate"
        description: "Job application success rate is {{ $value | humanizePercentage }} (threshold: 70%) over the last 30 minutes. This may indicate issues with the auto-apply service."
        runbook_url: "https://runbooks.jobpilot.com/alerts/LowApplicationSuccessRate"
        dashboard_url: "https://grafana.jobpilot.com/d/business-metrics"

    # Warning: AI service rate limited
    - alert: AIServiceRateLimited
      expr: rate(ai_service_rate_limited_total{namespace="jobpilot"}[5m]) * 60 > 10
      for: 5m
      labels:
        severity: warning
        category: business
        team: ai
      annotations:
        summary: "AI service is being rate limited"
        description: "AI service is experiencing {{ $value }} rate-limited requests per minute. This may impact user experience."
        runbook_url: "https://runbooks.jobpilot.com/alerts/AIServiceRateLimited"
        dashboard_url: "https://grafana.jobpilot.com/d/ai-service"

    # Warning: High user churn rate
    - alert: HighUserChurnRate
      expr: |
        (
          sum(increase(user_churn_total{namespace="jobpilot"}[24h]))
          /
          sum(user_active_total{namespace="jobpilot"})
        ) * 100 > 10
      for: 1h
      labels:
        severity: warning
        category: business
        team: product
      annotations:
        summary: "High user churn rate detected"
        description: "User churn rate is {{ $value | humanizePercentage }} in the last 24 hours (threshold: 10%). This requires immediate attention."
        runbook_url: "https://runbooks.jobpilot.com/alerts/HighUserChurnRate"
        dashboard_url: "https://grafana.jobpilot.com/d/user-metrics"

    # Critical: High payment failure rate
    - alert: PaymentFailureRate
      expr: |
        (
          sum(rate(payment_transactions_total{status="failed",namespace="jobpilot"}[15m]))
          /
          sum(rate(payment_transactions_total{namespace="jobpilot"}[15m]))
        ) * 100 > 5
      for: 15m
      labels:
        severity: critical
        category: business
        team: billing
      annotations:
        summary: "High payment failure rate"
        description: "Payment failure rate is {{ $value | humanizePercentage }} over the last 15 minutes (threshold: 5%). This is impacting revenue."
        runbook_url: "https://runbooks.jobpilot.com/alerts/PaymentFailureRate"
        dashboard_url: "https://grafana.jobpilot.com/d/payments"

    # Warning: Low resume generation success rate
    - alert: LowResumeGenerationSuccessRate
      expr: |
        (
          sum(rate(resume_generation_total{status="success",namespace="jobpilot"}[15m]))
          /
          sum(rate(resume_generation_total{namespace="jobpilot"}[15m]))
        ) * 100 < 80
      for: 15m
      labels:
        severity: warning
        category: business
        team: ai
      annotations:
        summary: "Low resume generation success rate"
        description: "Resume generation success rate is {{ $value | humanizePercentage }} (threshold: 80%) over the last 15 minutes."
        runbook_url: "https://runbooks.jobpilot.com/alerts/LowResumeGenerationSuccessRate"
        dashboard_url: "https://grafana.jobpilot.com/d/resume-service"

    # Warning: High API quota exhaustion
    - alert: HighAPIQuotaExhaustion
      expr: rate(api_quota_exhausted_total{namespace="jobpilot"}[5m]) > 1
      for: 10m
      labels:
        severity: warning
        category: business
        team: platform
      annotations:
        summary: "High API quota exhaustion rate"
        description: "Users are hitting API quota limits at {{ $value }} events per second. Consider reviewing quota policies."
        runbook_url: "https://runbooks.jobpilot.com/alerts/HighAPIQuotaExhaustion"
        dashboard_url: "https://grafana.jobpilot.com/d/api-usage"

    # Warning: Slow job search response time
    - alert: SlowJobSearchResponseTime
      expr: |
        histogram_quantile(0.95,
          sum(rate(job_search_duration_seconds_bucket{namespace="jobpilot"}[5m])) by (le)
        ) > 3
      for: 10m
      labels:
        severity: warning
        category: performance
        team: search
      annotations:
        summary: "Slow job search response time"
        description: "Job search P95 latency is {{ $value | humanizeDuration }} (threshold: 3s). User experience may be degraded."
        runbook_url: "https://runbooks.jobpilot.com/alerts/SlowJobSearchResponseTime"
        dashboard_url: "https://grafana.jobpilot.com/d/job-service"
