# ============================================================================
# Self-Healing Agent Configuration
# ============================================================================
# This pipeline acts as an autonomous self-healing agent that:
#   A. Detects problems automatically
#   B. Fixes common issues without asking
#   C. Prevents future breakage
#   D. Performs long-term auto-maintenance
# ============================================================================

trigger: none

schedules:
  # Run every 30 minutes for continuous self-healing
  - cron: '*/30 * * * *'
    displayName: 'Self-Healing Check'
    branches:
      include:
        - main
        - develop
    always: true

pool:
  name: 'Default'

variables:
  - group: applyplatform-common
  - name: ACR_NAME
    value: 'applyforusacr'
  - name: AKS_CLUSTER_NAME
    value: 'applyforus-aks'
  - name: RESOURCE_GROUP
    value: 'applyforus-prod-rg'
  - name: AUTO_HEAL_ENABLED
    value: 'true'
  - name: MAX_AUTO_RESTARTS
    value: '3'
  - name: SCALE_THRESHOLD_CPU
    value: '80'
  - name: SCALE_THRESHOLD_MEMORY
    value: '85'

stages:
  # ============================================================================
  # Stage A: Problem Detection
  # ============================================================================
  - stage: ProblemDetection
    displayName: 'A. Detect Problems'
    jobs:
      - job: DetectIssues
        displayName: 'Scan for Issues'
        steps:
          - checkout: none

          - task: AzureCLI@2
            displayName: 'Detect Pod Issues'
            name: DetectPods
            inputs:
              azureSubscription: 'ApplyPlatform'
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                echo "=== Self-Healing Agent: Problem Detection ==="
                echo "Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
                echo ""

                # Get AKS credentials
                az aks get-credentials \
                  --resource-group $(RESOURCE_GROUP) \
                  --name $(AKS_CLUSTER_NAME) \
                  --overwrite-existing

                # Initialize tracking
                ISSUES_FOUND=0
                ISSUES_JSON='{"pods":[],"deployments":[],"services":[],"nodes":[]}'

                ENVIRONMENTS="dev test prod"

                for env in $ENVIRONMENTS; do
                  NAMESPACE="applyforus-$env"
                  echo ""
                  echo ">>> Scanning $NAMESPACE..."

                  # Check for CrashLoopBackOff pods
                  CRASH_PODS=$(kubectl get pods -n $NAMESPACE -o json | \
                    jq -r '.items[] | select(.status.containerStatuses[]?.state.waiting.reason == "CrashLoopBackOff") | .metadata.name' 2>/dev/null || echo "")

                  if [ -n "$CRASH_PODS" ]; then
                    echo "  CRASH LOOPS: $CRASH_PODS"
                    ISSUES_FOUND=$((ISSUES_FOUND + 1))
                    echo "##vso[task.setvariable variable=CRASH_PODS_${env^^};isOutput=true]$CRASH_PODS"
                    echo "##vso[task.setvariable variable=HAS_CRASH_LOOPS;isOutput=true]true"
                  fi

                  # Check for ImagePullBackOff
                  IMAGE_PULL_PODS=$(kubectl get pods -n $NAMESPACE -o json | \
                    jq -r '.items[] | select(.status.containerStatuses[]?.state.waiting.reason == "ImagePullBackOff") | .metadata.name' 2>/dev/null || echo "")

                  if [ -n "$IMAGE_PULL_PODS" ]; then
                    echo "  IMAGE PULL ERRORS: $IMAGE_PULL_PODS"
                    ISSUES_FOUND=$((ISSUES_FOUND + 1))
                    echo "##vso[task.setvariable variable=IMAGE_PULL_${env^^};isOutput=true]$IMAGE_PULL_PODS"
                    echo "##vso[task.setvariable variable=HAS_IMAGE_PULL;isOutput=true]true"
                  fi

                  # Check for pending pods (stuck scheduling)
                  PENDING_PODS=$(kubectl get pods -n $NAMESPACE --field-selector=status.phase=Pending -o json | \
                    jq -r '.items[].metadata.name' 2>/dev/null || echo "")

                  if [ -n "$PENDING_PODS" ]; then
                    echo "  PENDING PODS: $PENDING_PODS"
                    ISSUES_FOUND=$((ISSUES_FOUND + 1))
                    echo "##vso[task.setvariable variable=PENDING_${env^^};isOutput=true]$PENDING_PODS"
                    echo "##vso[task.setvariable variable=HAS_PENDING;isOutput=true]true"
                  fi

                  # Check for OOMKilled containers
                  OOM_PODS=$(kubectl get pods -n $NAMESPACE -o json | \
                    jq -r '.items[] | select(.status.containerStatuses[]?.lastState.terminated.reason == "OOMKilled") | .metadata.name' 2>/dev/null || echo "")

                  if [ -n "$OOM_PODS" ]; then
                    echo "  OOM KILLED: $OOM_PODS"
                    ISSUES_FOUND=$((ISSUES_FOUND + 1))
                    echo "##vso[task.setvariable variable=OOM_${env^^};isOutput=true]$OOM_PODS"
                    echo "##vso[task.setvariable variable=HAS_OOM;isOutput=true]true"
                  fi

                  # Check deployment health
                  UNHEALTHY_DEPLOYS=$(kubectl get deployments -n $NAMESPACE -o json | \
                    jq -r '.items[] | select(.status.availableReplicas < .spec.replicas) | .metadata.name' 2>/dev/null || echo "")

                  if [ -n "$UNHEALTHY_DEPLOYS" ]; then
                    echo "  UNHEALTHY DEPLOYMENTS: $UNHEALTHY_DEPLOYS"
                    ISSUES_FOUND=$((ISSUES_FOUND + 1))
                    echo "##vso[task.setvariable variable=UNHEALTHY_DEPLOYS_${env^^};isOutput=true]$UNHEALTHY_DEPLOYS"
                    echo "##vso[task.setvariable variable=HAS_UNHEALTHY_DEPLOYS;isOutput=true]true"
                  fi
                done

                echo ""
                echo "Total issues detected: $ISSUES_FOUND"
                echo "##vso[task.setvariable variable=ISSUES_FOUND;isOutput=true]$ISSUES_FOUND"

          - task: AzureCLI@2
            displayName: 'Detect Infrastructure Issues'
            name: DetectInfra
            inputs:
              azureSubscription: 'ApplyPlatform'
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                echo "=== Infrastructure Issue Detection ==="

                INFRA_ISSUES=0

                # Check node health
                echo ""
                echo ">>> Node Health:"
                UNHEALTHY_NODES=$(kubectl get nodes -o json | \
                  jq -r '.items[] | select(.status.conditions[] | select(.type=="Ready" and .status!="True")) | .metadata.name' 2>/dev/null || echo "")

                if [ -n "$UNHEALTHY_NODES" ]; then
                  echo "UNHEALTHY NODES: $UNHEALTHY_NODES"
                  INFRA_ISSUES=$((INFRA_ISSUES + 1))
                  echo "##vso[task.setvariable variable=UNHEALTHY_NODES;isOutput=true]$UNHEALTHY_NODES"
                  echo "##vso[task.setvariable variable=HAS_NODE_ISSUES;isOutput=true]true"
                else
                  echo "All nodes healthy"
                fi

                # Check disk pressure
                echo ""
                echo ">>> Disk Pressure:"
                DISK_PRESSURE=$(kubectl get nodes -o json | \
                  jq -r '.items[] | select(.status.conditions[] | select(.type=="DiskPressure" and .status=="True")) | .metadata.name' 2>/dev/null || echo "")

                if [ -n "$DISK_PRESSURE" ]; then
                  echo "DISK PRESSURE: $DISK_PRESSURE"
                  INFRA_ISSUES=$((INFRA_ISSUES + 1))
                  echo "##vso[task.setvariable variable=DISK_PRESSURE_NODES;isOutput=true]$DISK_PRESSURE"
                  echo "##vso[task.setvariable variable=HAS_DISK_PRESSURE;isOutput=true]true"
                else
                  echo "No disk pressure"
                fi

                # Check memory pressure
                echo ""
                echo ">>> Memory Pressure:"
                MEMORY_PRESSURE=$(kubectl get nodes -o json | \
                  jq -r '.items[] | select(.status.conditions[] | select(.type=="MemoryPressure" and .status=="True")) | .metadata.name' 2>/dev/null || echo "")

                if [ -n "$MEMORY_PRESSURE" ]; then
                  echo "MEMORY PRESSURE: $MEMORY_PRESSURE"
                  INFRA_ISSUES=$((INFRA_ISSUES + 1))
                  echo "##vso[task.setvariable variable=MEMORY_PRESSURE_NODES;isOutput=true]$MEMORY_PRESSURE"
                  echo "##vso[task.setvariable variable=HAS_MEMORY_PRESSURE;isOutput=true]true"
                else
                  echo "No memory pressure"
                fi

                # Check PVC issues
                echo ""
                echo ">>> PVC Status:"
                PENDING_PVC=$(kubectl get pvc -A -o json | \
                  jq -r '.items[] | select(.status.phase=="Pending") | "\(.metadata.namespace)/\(.metadata.name)"' 2>/dev/null || echo "")

                if [ -n "$PENDING_PVC" ]; then
                  echo "PENDING PVCs: $PENDING_PVC"
                  INFRA_ISSUES=$((INFRA_ISSUES + 1))
                  echo "##vso[task.setvariable variable=PENDING_PVC;isOutput=true]$PENDING_PVC"
                fi

                echo ""
                echo "Infrastructure issues: $INFRA_ISSUES"
                echo "##vso[task.setvariable variable=INFRA_ISSUES;isOutput=true]$INFRA_ISSUES"

  # ============================================================================
  # Stage B: Auto-Fix Issues
  # ============================================================================
  - stage: AutoFix
    displayName: 'B. Fix Issues Automatically'
    dependsOn: ProblemDetection
    condition: |
      and(
        succeeded(),
        eq(variables['AUTO_HEAL_ENABLED'], 'true'),
        or(
          eq(dependencies.ProblemDetection.outputs['DetectPods.HAS_CRASH_LOOPS'], 'true'),
          eq(dependencies.ProblemDetection.outputs['DetectPods.HAS_OOM'], 'true'),
          eq(dependencies.ProblemDetection.outputs['DetectPods.HAS_UNHEALTHY_DEPLOYS'], 'true'),
          eq(dependencies.ProblemDetection.outputs['DetectInfra.HAS_NODE_ISSUES'], 'true')
        )
      )
    variables:
      HAS_CRASH_LOOPS: $[ dependencies.ProblemDetection.outputs['DetectPods.HAS_CRASH_LOOPS'] ]
      HAS_OOM: $[ dependencies.ProblemDetection.outputs['DetectPods.HAS_OOM'] ]
      HAS_UNHEALTHY_DEPLOYS: $[ dependencies.ProblemDetection.outputs['DetectPods.HAS_UNHEALTHY_DEPLOYS'] ]
      CRASH_PODS_DEV: $[ dependencies.ProblemDetection.outputs['DetectPods.CRASH_PODS_DEV'] ]
      CRASH_PODS_TEST: $[ dependencies.ProblemDetection.outputs['DetectPods.CRASH_PODS_TEST'] ]
      CRASH_PODS_PROD: $[ dependencies.ProblemDetection.outputs['DetectPods.CRASH_PODS_PROD'] ]
    jobs:
      - job: FixCrashLoops
        displayName: 'Fix Crash Loops'
        condition: eq(variables['HAS_CRASH_LOOPS'], 'true')
        steps:
          - checkout: none

          - task: AzureCLI@2
            displayName: 'Restart Crashing Pods'
            inputs:
              azureSubscription: 'ApplyPlatform'
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                echo "=== Auto-Fix: Crash Loop Recovery ==="

                az aks get-credentials \
                  --resource-group $(RESOURCE_GROUP) \
                  --name $(AKS_CLUSTER_NAME) \
                  --overwrite-existing

                ENVIRONMENTS="dev test prod"

                for env in $ENVIRONMENTS; do
                  NAMESPACE="applyforus-$env"

                  # Get crashing pods
                  CRASH_PODS=$(kubectl get pods -n $NAMESPACE -o json | \
                    jq -r '.items[] | select(.status.containerStatuses[]?.state.waiting.reason == "CrashLoopBackOff") | .metadata.name' 2>/dev/null || echo "")

                  if [ -n "$CRASH_PODS" ]; then
                    echo ""
                    echo ">>> Fixing crash loops in $NAMESPACE"

                    for pod in $CRASH_PODS; do
                      echo "  Deleting pod: $pod"

                      # Get the deployment that owns this pod
                      OWNER=$(kubectl get pod $pod -n $NAMESPACE -o jsonpath='{.metadata.ownerReferences[0].name}' 2>/dev/null || echo "")

                      # Delete the pod to trigger recreation
                      kubectl delete pod $pod -n $NAMESPACE --grace-period=30 || true

                      # Check restart count
                      if [ -n "$OWNER" ]; then
                        RESTART_COUNT=$(kubectl get deployment $OWNER -n $NAMESPACE -o jsonpath='{.metadata.annotations.self-healing-restarts}' 2>/dev/null || echo "0")

                        if [ "$RESTART_COUNT" -lt "$(MAX_AUTO_RESTARTS)" ]; then
                          NEW_COUNT=$((RESTART_COUNT + 1))
                          kubectl annotate deployment $OWNER -n $NAMESPACE self-healing-restarts=$NEW_COUNT --overwrite || true
                          echo "  Restart count: $NEW_COUNT/$(MAX_AUTO_RESTARTS)"
                        else
                          echo "  ##vso[task.logissue type=warning]Max restarts reached for $OWNER - manual intervention required"
                        fi
                      fi
                    done
                  fi
                done

      - job: FixOOMIssues
        displayName: 'Fix OOM Issues'
        condition: eq(variables['HAS_OOM'], 'true')
        steps:
          - checkout: none

          - task: AzureCLI@2
            displayName: 'Adjust Memory Limits'
            inputs:
              azureSubscription: 'ApplyPlatform'
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                echo "=== Auto-Fix: OOM Recovery ==="

                az aks get-credentials \
                  --resource-group $(RESOURCE_GROUP) \
                  --name $(AKS_CLUSTER_NAME) \
                  --overwrite-existing

                ENVIRONMENTS="dev test prod"

                for env in $ENVIRONMENTS; do
                  NAMESPACE="applyforus-$env"

                  # Get OOM killed pods
                  OOM_PODS=$(kubectl get pods -n $NAMESPACE -o json | \
                    jq -r '.items[] | select(.status.containerStatuses[]?.lastState.terminated.reason == "OOMKilled") | .metadata.name' 2>/dev/null || echo "")

                  if [ -n "$OOM_PODS" ]; then
                    echo ""
                    echo ">>> Handling OOM in $NAMESPACE"

                    for pod in $OOM_PODS; do
                      echo "  Pod with OOM: $pod"

                      # Get deployment name
                      DEPLOYMENT=$(kubectl get pod $pod -n $NAMESPACE -o jsonpath='{.metadata.ownerReferences[?(@.kind=="ReplicaSet")].name}' 2>/dev/null | sed 's/-[^-]*$//')

                      if [ -n "$DEPLOYMENT" ]; then
                        # Get current memory limit
                        CURRENT_LIMIT=$(kubectl get deployment $DEPLOYMENT -n $NAMESPACE -o jsonpath='{.spec.template.spec.containers[0].resources.limits.memory}' 2>/dev/null || echo "512Mi")

                        echo "  Current memory limit: $CURRENT_LIMIT"
                        echo "  ##vso[task.logissue type=warning]OOM detected in $DEPLOYMENT - consider increasing memory limit from $CURRENT_LIMIT"

                        # Restart the pod to clear OOM state
                        kubectl delete pod $pod -n $NAMESPACE --grace-period=30 || true
                      fi
                    done
                  fi
                done

      - job: FixUnhealthyDeployments
        displayName: 'Fix Unhealthy Deployments'
        condition: eq(variables['HAS_UNHEALTHY_DEPLOYS'], 'true')
        steps:
          - checkout: none

          - task: AzureCLI@2
            displayName: 'Rollback or Restart Deployments'
            inputs:
              azureSubscription: 'ApplyPlatform'
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                echo "=== Auto-Fix: Unhealthy Deployment Recovery ==="

                az aks get-credentials \
                  --resource-group $(RESOURCE_GROUP) \
                  --name $(AKS_CLUSTER_NAME) \
                  --overwrite-existing

                ENVIRONMENTS="dev test prod"

                for env in $ENVIRONMENTS; do
                  NAMESPACE="applyforus-$env"

                  # Get unhealthy deployments
                  UNHEALTHY=$(kubectl get deployments -n $NAMESPACE -o json | \
                    jq -r '.items[] | select(.status.availableReplicas < .spec.replicas or .status.availableReplicas == null) | .metadata.name' 2>/dev/null || echo "")

                  if [ -n "$UNHEALTHY" ]; then
                    echo ""
                    echo ">>> Fixing unhealthy deployments in $NAMESPACE"

                    for deploy in $UNHEALTHY; do
                      echo "  Deployment: $deploy"

                      # Check if there's a previous revision to rollback to
                      REVISION_COUNT=$(kubectl rollout history deployment/$deploy -n $NAMESPACE 2>/dev/null | grep -c "^[0-9]" || echo "0")

                      if [ "$REVISION_COUNT" -gt 1 ]; then
                        echo "  Attempting rollback..."
                        kubectl rollout undo deployment/$deploy -n $NAMESPACE || true
                      else
                        echo "  Restarting deployment..."
                        kubectl rollout restart deployment/$deploy -n $NAMESPACE || true
                      fi

                      # Wait for rollout
                      kubectl rollout status deployment/$deploy -n $NAMESPACE --timeout=300s || true
                    done
                  fi
                done

  # ============================================================================
  # Stage C: Prevention & Hardening
  # ============================================================================
  - stage: Prevention
    displayName: 'C. Prevent Future Breakage'
    dependsOn:
      - ProblemDetection
      - AutoFix
    condition: always()
    jobs:
      - job: ApplyPreventiveMeasures
        displayName: 'Apply Preventive Measures'
        steps:
          - checkout: none

          - task: AzureCLI@2
            displayName: 'Ensure Pod Disruption Budgets'
            inputs:
              azureSubscription: 'ApplyPlatform'
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                echo "=== Prevention: Pod Disruption Budgets ==="

                az aks get-credentials \
                  --resource-group $(RESOURCE_GROUP) \
                  --name $(AKS_CLUSTER_NAME) \
                  --overwrite-existing

                SERVICES="auth-service user-service job-service resume-service notification-service ai-service auto-apply-service analytics-service orchestrator-service web"
                NAMESPACE="applyforus-prod"

                for service in $SERVICES; do
                  # Check if PDB exists
                  PDB_EXISTS=$(kubectl get pdb $service-pdb -n $NAMESPACE 2>/dev/null || echo "")

                  if [ -z "$PDB_EXISTS" ]; then
                    echo "Creating PDB for $service"

                    kubectl apply -f - <<EOF
                apiVersion: policy/v1
                kind: PodDisruptionBudget
                metadata:
                  name: $service-pdb
                  namespace: $NAMESPACE
                spec:
                  minAvailable: 1
                  selector:
                    matchLabels:
                      app: $service
                EOF
                  else
                    echo "PDB exists for $service"
                  fi
                done
            continueOnError: true

          - task: AzureCLI@2
            displayName: 'Verify Resource Quotas'
            inputs:
              azureSubscription: 'ApplyPlatform'
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                echo "=== Prevention: Resource Quotas ==="

                az aks get-credentials \
                  --resource-group $(RESOURCE_GROUP) \
                  --name $(AKS_CLUSTER_NAME) \
                  --overwrite-existing

                ENVIRONMENTS="dev test prod"

                for env in $ENVIRONMENTS; do
                  NAMESPACE="applyforus-$env"
                  echo ""
                  echo ">>> Checking quotas in $NAMESPACE"

                  kubectl get resourcequota -n $NAMESPACE 2>/dev/null || echo "No resource quotas defined"
                  kubectl get limitrange -n $NAMESPACE 2>/dev/null || echo "No limit ranges defined"
                done
            continueOnError: true

  # ============================================================================
  # Stage D: Long-Term Maintenance
  # ============================================================================
  - stage: Maintenance
    displayName: 'D. Long-Term Maintenance'
    dependsOn: Prevention
    condition: always()
    jobs:
      - job: CleanupOldResources
        displayName: 'Cleanup Old Resources'
        steps:
          - checkout: none

          - task: AzureCLI@2
            displayName: 'Clean Old Images from ACR'
            inputs:
              azureSubscription: 'ApplyPlatform'
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                echo "=== Maintenance: ACR Cleanup ==="

                # Keep last 10 tags per repository
                SERVICES="web auth-service user-service job-service resume-service notification-service analytics-service ai-service auto-apply-service orchestrator-service"

                for service in $SERVICES; do
                  IMAGE="applyai-${service}"
                  echo ""
                  echo ">>> Cleaning $IMAGE"

                  # Get all tags sorted by time
                  TAGS=$(az acr repository show-tags \
                    --name $(ACR_NAME) \
                    --repository $IMAGE \
                    --orderby time_asc \
                    --output tsv 2>/dev/null || echo "")

                  TAG_COUNT=$(echo "$TAGS" | wc -l)

                  if [ "$TAG_COUNT" -gt 10 ]; then
                    # Keep last 10, delete the rest
                    DELETE_COUNT=$((TAG_COUNT - 10))
                    echo "  Found $TAG_COUNT tags, deleting oldest $DELETE_COUNT"

                    TAGS_TO_DELETE=$(echo "$TAGS" | head -n $DELETE_COUNT)

                    for tag in $TAGS_TO_DELETE; do
                      if [ "$tag" != "latest" ]; then
                        echo "    Deleting: $IMAGE:$tag"
                        az acr repository delete \
                          --name $(ACR_NAME) \
                          --image $IMAGE:$tag \
                          --yes 2>/dev/null || true
                      fi
                    done
                  else
                    echo "  $TAG_COUNT tags (no cleanup needed)"
                  fi
                done
            continueOnError: true

          - task: AzureCLI@2
            displayName: 'Clean Completed Jobs'
            inputs:
              azureSubscription: 'ApplyPlatform'
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                echo "=== Maintenance: Job Cleanup ==="

                az aks get-credentials \
                  --resource-group $(RESOURCE_GROUP) \
                  --name $(AKS_CLUSTER_NAME) \
                  --overwrite-existing

                ENVIRONMENTS="dev test prod"

                for env in $ENVIRONMENTS; do
                  NAMESPACE="applyforus-$env"
                  echo ""
                  echo ">>> Cleaning jobs in $NAMESPACE"

                  # Delete completed jobs older than 24 hours
                  kubectl delete jobs -n $NAMESPACE \
                    --field-selector status.successful=1 \
                    2>/dev/null || echo "No completed jobs to clean"

                  # Delete failed jobs older than 7 days
                  kubectl get jobs -n $NAMESPACE -o json | \
                    jq -r '.items[] | select(.status.conditions[]?.type=="Failed") | .metadata.name' | \
                    xargs -r kubectl delete job -n $NAMESPACE 2>/dev/null || echo "No failed jobs to clean"
                done
            continueOnError: true

          - task: AzureCLI@2
            displayName: 'Clean Evicted Pods'
            inputs:
              azureSubscription: 'ApplyPlatform'
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                echo "=== Maintenance: Evicted Pod Cleanup ==="

                az aks get-credentials \
                  --resource-group $(RESOURCE_GROUP) \
                  --name $(AKS_CLUSTER_NAME) \
                  --overwrite-existing

                # Delete all evicted pods across all namespaces
                kubectl get pods -A -o json | \
                  jq -r '.items[] | select(.status.reason=="Evicted") | "\(.metadata.namespace) \(.metadata.name)"' | \
                  while read ns name; do
                    echo "Deleting evicted pod: $ns/$name"
                    kubectl delete pod $name -n $ns --force --grace-period=0 2>/dev/null || true
                  done

                echo "Evicted pods cleanup complete"
            continueOnError: true

      - job: GenerateHealthReport
        displayName: 'Generate Health Report'
        dependsOn: CleanupOldResources
        steps:
          - checkout: none

          - script: |
              echo "=== Self-Healing Agent: Report ==="

              mkdir -p $(Build.ArtifactStagingDirectory)/health-report

              cat > $(Build.ArtifactStagingDirectory)/health-report/self-healing-report.json << EOF
              {
                "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
                "buildId": "$(Build.BuildId)",
                "agent": "self-healing",
                "version": "1.0.0",
                "status": "completed",
                "stages": {
                  "problemDetection": "completed",
                  "autoFix": "$([ \"$(AUTO_FIX_RAN)\" = \"true\" ] && echo 'executed' || echo 'skipped')",
                  "prevention": "completed",
                  "maintenance": "completed"
                }
              }
              EOF

              echo "Health report generated"
            displayName: 'Generate Report'

          - task: PublishPipelineArtifact@1
            displayName: 'Publish Health Report'
            inputs:
              targetPath: '$(Build.ArtifactStagingDirectory)/health-report'
              artifact: 'self-healing-report-$(Build.BuildId)'
              publishLocation: 'pipeline'

          - script: |
              echo "=============================================="
              echo "    SELF-HEALING AGENT CYCLE COMPLETE"
              echo "=============================================="
              echo ""
              echo "Agent: Pipeline Self-Healing Agent v1.0"
              echo "Build: $(Build.BuildId)"
              echo "Time: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
              echo ""
              echo "Capabilities:"
              echo "  [x] Problem Detection"
              echo "  [x] Auto-Fix (crash loops, OOM, unhealthy deploys)"
              echo "  [x] Prevention (PDBs, quotas)"
              echo "  [x] Maintenance (ACR cleanup, job cleanup)"
              echo ""
              echo "Next cycle: Scheduled per cron configuration"
              echo "=============================================="
            displayName: 'Agent Summary'
